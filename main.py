#!/usr/bin/env python3
"""é£²é£Ÿåº—å£ã‚³ãƒŸåé›†ãƒ»åˆ†æãƒ„ãƒ¼ãƒ« ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆã€‚"""

import argparse
import asyncio
import json
import os
import sys

import nest_asyncio

nest_asyncio.apply()


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="é£²é£Ÿåº—å£ã‚³ãƒŸåé›†ãƒ»åˆ†æãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""ä½¿ç”¨ä¾‹:
  python main.py --name "ãƒ†ã‚¹ãƒˆé£Ÿå ‚" --tripadvisor "https://www.tripadvisor.jp/..."
  python main.py --name "ãƒ†ã‚¹ãƒˆé£Ÿå ‚" --google-maps "https://www.google.com/maps/..." --tabelog "https://tabelog.com/..."
  python main.py --name "ãƒ†ã‚¹ãƒˆé£Ÿå ‚" --skip-scrape   # æ—¢å­˜JSONã‹ã‚‰åˆ†æã®ã¿å†å®Ÿè¡Œ
""",
    )
    parser.add_argument("--name", default="åº—èˆ—", help="åº—èˆ—åï¼ˆãƒ¬ãƒãƒ¼ãƒˆã®ã‚¿ã‚¤ãƒˆãƒ«ã«ä½¿ç”¨ï¼‰")
    parser.add_argument("--google-maps", dest="google_maps", metavar="URL", help="Google ãƒãƒƒãƒ— URL")
    parser.add_argument("--tabelog", metavar="URL", help="é£Ÿã¹ãƒ­ã‚° URL")
    parser.add_argument("--tripadvisor", metavar="URL", help="TripAdvisor URL")
    parser.add_argument(
        "--skip-scrape",
        action="store_true",
        help="ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æ—¢å­˜ã® reviews_raw.json ã‹ã‚‰åˆ†æã®ã¿å®Ÿè¡Œ",
    )
    parser.add_argument("--output", default="report.html", help="å‡ºåŠ› HTML ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: report.htmlï¼‰")
    return parser.parse_args()


async def run_scrapers(args: argparse.Namespace) -> list[dict]:
    """æŒ‡å®šã•ã‚ŒãŸ URL ã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã€å…¨å£ã‚³ãƒŸã‚’è¿”ã™ã€‚"""
    from scrapers import scrape_google_maps, scrape_tabelog, scrape_tripadvisor

    all_reviews: list[dict] = []

    if args.google_maps:
        try:
            reviews = await scrape_google_maps(args.google_maps)
            all_reviews.extend(reviews)
        except Exception as e:
            print(f"âš ï¸ Google ãƒãƒƒãƒ— ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

    if args.tabelog:
        try:
            reviews = await scrape_tabelog(args.tabelog)
            all_reviews.extend(reviews)
        except Exception as e:
            print(f"âš ï¸ é£Ÿã¹ãƒ­ã‚° ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

    if args.tripadvisor:
        try:
            reviews = await scrape_tripadvisor(args.tripadvisor)
            all_reviews.extend(reviews)
        except Exception as e:
            print(f"âš ï¸ TripAdvisor ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")

    return all_reviews


def main() -> None:
    args = parse_args()

    # URL ã‚‚ã‚¹ã‚­ãƒƒãƒ—ãƒ•ãƒ©ã‚°ã‚‚æŒ‡å®šãªã—
    if not args.skip_scrape and not any([args.google_maps, args.tabelog, args.tripadvisor]):
        print("âŒ ã‚¨ãƒ©ãƒ¼: --google-maps / --tabelog / --tripadvisor ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        print("   æ—¢å­˜ JSON ã‹ã‚‰å†åˆ†æã™ã‚‹å ´åˆã¯ --skip-scrape ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)

    raw_json_path = "reviews_raw.json"
    analyzed_json_path = "reviews_analyzed.json"

    # ---- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ----
    if args.skip_scrape:
        if not os.path.exists(raw_json_path):
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {raw_json_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)
        print(f"â­ï¸  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã€‚{raw_json_path} ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
        with open(raw_json_path, encoding="utf-8") as f:
            all_reviews = json.load(f)
        print(f"  ğŸ“‚ {len(all_reviews)}ä»¶ã®å£ã‚³ãƒŸã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    else:
        print(f"\nğŸš€ å£ã‚³ãƒŸåé›†ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆåº—èˆ—å: {args.name}ï¼‰\n")
        all_reviews = asyncio.run(run_scrapers(args))

        if not all_reviews:
            print("âš ï¸ å£ã‚³ãƒŸãŒ1ä»¶ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚URL ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)

        with open(raw_json_path, "w", encoding="utf-8") as f:
            json.dump(all_reviews, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ {len(all_reviews)}ä»¶ã®å£ã‚³ãƒŸã‚’ {raw_json_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    # ---- Gemini åˆ†æ ----
    from analyzer import analyze_reviews

    analysis = analyze_reviews(all_reviews)

    with open(analyzed_json_path, "w", encoding="utf-8") as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ åˆ†æçµæœã‚’ {analyzed_json_path} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    # ---- HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ----
    from reporter import generate_report

    generate_report(args.name, analysis, output_path=args.output)


if __name__ == "__main__":
    main()
