"""Gemini ã«ã‚ˆã‚‹å£ã‚³ãƒŸåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚"""

import json
import os
import re
import time
from collections import Counter, defaultdict
from datetime import datetime

from google import genai
from dotenv import load_dotenv

load_dotenv()

_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY", ""))
MODEL = "gemini-2.5-flash"

KANDO_TYPES = ["threshold", "surprise", "resonance", "rescue", "awe", "participation", "growth"]
KANDO_LABELS = {
    "threshold":    "â‘ ã—ãã„å€¤çªç ´",
    "surprise":     "â‘¡æ„å¤–æ€§",
    "resonance":    "â‘¢å…±é³´ãƒ»å…±æ„Ÿ",
    "rescue":       "â‘£æ•‘æ¸ˆ",
    "awe":          "â‘¤å´‡é«˜",
    "participation":"â‘¥å‚åŠ ",
    "growth":       "â‘¦æˆé•·",
}


def _generate(prompt: str) -> str:
    response = _client.models.generate_content(model=MODEL, contents=prompt)
    return response.text


# ---------------------------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒª
# ---------------------------------------------------------------------------

def _clean_text(text: str) -> str:
    """Google ãƒãƒƒãƒ—ãŒä»˜åŠ ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆé£Ÿäº‹ã®ç¨®é¡ãƒ»æ–™é‡‘ãƒ»è©•ç‚¹ãªã©ï¼‰ã‚’æœ¬æ–‡ã‹ã‚‰é™¤å»ã™ã‚‹ã€‚"""
    metadata_markers = [
        r'é£Ÿäº‹ã®ç¨®é¡',
        r'1\s*äººã‚ãŸã‚Šã®æ–™é‡‘',
        r'é£Ÿäº‹[:ï¼š]\s*\d',
        r'ã‚µãƒ¼ãƒ“ã‚¹[:ï¼š]\s*\d',
        r'é›°å›²æ°—[:ï¼š]\s*\d',
        r'äºˆç´„\n',
        r'ã‚°ãƒ«ãƒ¼ãƒ—ã®äººæ•°',
    ]
    pattern = '|'.join(f'(?:{m})' for m in metadata_markers)
    m = re.search(pattern, text)
    if m:
        text = text[:m.start()].strip()
    return text


def analyze_reviews(reviews: list[dict]) -> dict:
    # ä¿å­˜æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿å†…ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»ï¼ˆ--skip-scrape æ™‚ã‚‚å¯¾å¿œï¼‰
    reviews = [dict(r, text=_clean_text(r.get("text", ""))) for r in reviews]
    print(f"\nğŸ¤– Gemini åˆ†æé–‹å§‹ï¼ˆ{len(reviews)}ä»¶ï¼‰...")

    keywords = _extract_keywords(reviews)
    experience = _analyze_experience(reviews, keywords)
    timeseries_keywords = _analyze_timeseries_keywords(reviews, keywords)
    kando = _analyze_kando(reviews)

    return {
        "reviews": reviews,
        "keywords": keywords,
        "experience": experience,
        "timeseries_keywords": timeseries_keywords,
        "kando": kando,
    }


# ---------------------------------------------------------------------------
# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆGemini ãƒãƒƒãƒï¼‰
# ---------------------------------------------------------------------------

BATCH_SIZE = 20

def _extract_keywords(reviews: list[dict]) -> list[dict]:
    """Gemini ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®šã—ã€å®Ÿãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã§å‡ºç¾ä»¶æ•°ã‚’é›†è¨ˆã™ã‚‹ã€‚"""
    print("  ğŸ”‘ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºä¸­...")
    word_sentiments: dict[str, list[str]] = defaultdict(list)
    total_batches = (len(reviews) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(0, len(reviews), BATCH_SIZE):
        batch = reviews[i: i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1
        print(f"    ğŸ“¦ ãƒãƒƒãƒ {batch_num}/{total_batches}...")
        texts = "\n".join(f"[{j}] {r['text'][:200]}" for j, r in enumerate(batch))

        prompt = f"""ä»¥ä¸‹ã®é£²é£Ÿåº—å£ã‚³ãƒŸã‹ã‚‰ã€é¡§å®¢å¿ƒç†ãƒ»é¡§å®¢ä¾¡å€¤ã‚’è¡¨ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã€æŠ½å‡ºã™ã‚‹è¨€è‘‰ã€‘
- é¡§å®¢ã®æ„Ÿæƒ…ãƒ»è©•ä¾¡ãƒ»ä½“é¨“ä¾¡å€¤ã‚’è¡¨ã™è¨€è‘‰ï¼ˆä¾‹ï¼šç¾å‘³ã—ã„ã€æ„Ÿå‹•ã€æ˜ ãˆã€æœ€é«˜ã€æ®‹å¿µã€ã¾ãŸæ¥ãŸã„ã€ã‚³ã‚¹ãƒ‘ã€éæ—¥å¸¸ã€å¾…ã¡ã™ãã€é›°å›²æ°—æŠœç¾¤ï¼‰
- å½¢å®¹è©ãƒ»è©•ä¾¡å‹•è©ãƒ»å°è±¡ãƒ»æº€è¶³åº¦ã‚’è¡¨ã™è¡¨ç¾ã‚’å„ªå…ˆ

ã€é™¤å¤–ã™ã‚‹è¨€è‘‰ã€‘
- ã€Œæ–™ç†ã€ã€Œé£Ÿäº‹ã€ã€Œã‚¹ã‚¿ãƒƒãƒ•ã€ã€Œãƒ‰ãƒªãƒ³ã‚¯ã€ã€Œå¸­ã€ã€Œåº—å†…ã€ã€ŒãŠåº—ã€ã€Œãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€ã€Œæ³¨æ–‡ã€ã€Œãƒ†ãƒ¼ãƒ–ãƒ«ã€ã€Œãƒ©ãƒ³ãƒã€ã€Œãƒ‡ã‚£ãƒŠãƒ¼ã€ã€Œé£²ã¿ç‰©ã€ã€Œé£Ÿã¹ç‰©ã€ã€Œåº—å“¡ã€ã€Œåº—èˆ—ã€ãªã©ã€é£²é£Ÿåº—ã¨ã—ã¦å½“ãŸã‚Šå‰ã®ç‰©ãƒ»å ´æ‰€ãƒ»äººã‚’è¡¨ã™ä¸€èˆ¬åè©
- åŠ©è©ãƒ»æ¥ç¶šè©ãƒ»èªå°¾

ã€çµ±ä¸€ãƒ«ãƒ¼ãƒ«ã€‘
- é¡ä¼¼è¡¨ç¾ã¯ä»£è¡¨çš„ãªè¡¨è¨˜ã«çµ±ä¸€ï¼ˆä¾‹:ã€Œç¾å‘³ã—ã„ã€ã€ŒãŠã„ã—ã„ã€â†’ã€Œç¾å‘³ã—ã„ã€ï¼‰
- å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒã‚¸ãƒã‚¬ã‚‚åˆ¤å®š

å£ã‚³ãƒŸ:
{texts}

å‡ºåŠ›å½¢å¼ï¼ˆJSONã®ã¿ï¼‰:
[{{"word":"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰","sentiment":"positive|negative|neutral"}}]"""

        try:
            response_text = _generate(prompt)
            json_match = re.search(r"\[.*\]", response_text, re.DOTALL)
            if json_match:
                for item in json.loads(json_match.group()):
                    w = item.get("word", "").strip()
                    s = item.get("sentiment", "neutral")
                    if w and len(w) >= 2:
                        word_sentiments[w].append(s)
            time.sleep(2)
        except Exception as e:
            print(f"    âš ï¸ ã‚¨ãƒ©ãƒ¼ï¼ˆãƒãƒƒãƒ {batch_num}ï¼‰: {e}")

    # ä¸€èˆ¬åè©é™¤å¤–ãƒªã‚¹ãƒˆ
    GENERIC_WORDS = {
        "æ–™ç†", "é£Ÿäº‹", "ã‚¹ã‚¿ãƒƒãƒ•", "ãƒ‰ãƒªãƒ³ã‚¯", "å¸­", "åº—å†…", "ãŠåº—", "ãƒ¡ãƒ‹ãƒ¥ãƒ¼",
        "æ³¨æ–‡", "ãƒ†ãƒ¼ãƒ–ãƒ«", "ãƒ©ãƒ³ãƒ", "ãƒ‡ã‚£ãƒŠãƒ¼", "é£²ã¿ç‰©", "é£Ÿã¹ç‰©", "åº—å“¡", "åº—èˆ—",
        "ãŠæ–™ç†", "é£²é£Ÿ", "æœé£Ÿ", "æ˜¼é£Ÿ", "å¤•é£Ÿ", "å¤œã”é£¯", "æ˜¼ã”é£¯", "æœã”é£¯",
        "å…¥å£", "å‡ºå£", "å…¥ã‚Šå£", "ãƒˆã‚¤ãƒ¬", "é§è»Šå ´", "äºˆç´„", "ä¼šè¨ˆ", "ãƒ¬ã‚¸",
        "åº—", "æ–¹", "äºº", "æ™‚", "æ–¹ã€…", "çš†ã•ã‚“", "çš†æ§˜", "ã“ã¡ã‚‰", "ã“ã¨",
    }

    # ãƒã‚¸ãƒã‚¬ã‚’å¤šæ•°æ±ºã§æ±ºå®šã—ã€å®Ÿãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ã§å‡ºç¾ä»¶æ•°ã‚’é›†è¨ˆ
    all_texts = [r.get("text", "") for r in reviews]
    ranked = []
    for word, sents in word_sentiments.items():
        if word in GENERIC_WORDS:
            continue
        pos, neg = sents.count("positive"), sents.count("negative")
        sentiment = "positive" if pos > neg else "negative" if neg > pos else "neutral"
        count = sum(1 for t in all_texts if word in t)
        if count > 0:
            ranked.append({"word": word, "count": count, "sentiment": sentiment})

    ranked.sort(key=lambda x: -x["count"])
    return ranked[:50]


# ---------------------------------------------------------------------------
# é¡§å®¢ä½“é¨“ä¾¡å€¤åˆ†æ
# ---------------------------------------------------------------------------

def _analyze_experience(reviews: list[dict], keywords: list[dict]) -> dict:
    print("  âœ¨ é¡§å®¢ä½“é¨“ä¾¡å€¤ã‚’åˆ†æä¸­...")
    total = len(reviews)

    # ãƒã‚¸ãƒ»ãƒã‚¬ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åˆ†é›¢
    pos_kws = [k for k in keywords if k.get("sentiment") == "positive"]
    neg_kws = [k for k in keywords if k.get("sentiment") == "negative"]
    pos_summary = "ã€".join(f"{k['word']}({k['count']}ä»¶)" for k in pos_kws[:15])
    neg_summary = "ã€".join(f"{k['word']}({k['count']}ä»¶)" for k in neg_kws[:15])

    # ä½è©•ä¾¡ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–å£ã‚³ãƒŸã‚’å„ªå…ˆã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã«å«ã‚ã‚‹
    low_rated = [r for r in reviews if r.get("rating") is not None and float(r.get("rating", 5)) <= 3]
    high_rated = [r for r in reviews if r not in low_rated]
    # ä½è©•ä¾¡ã‚’æœ€å¤§15ä»¶ + é«˜è©•ä¾¡ã‹ã‚‰15ä»¶
    sample = low_rated[:15] + high_rated[:15]
    sample_text = "\n".join(f"[â˜…{r.get('rating','?')}] {r['text'][:200]}" for r in sample)

    prompt = f"""ä»¥ä¸‹ã®é£²é£Ÿåº—å£ã‚³ãƒŸãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€å®¢è¦³çš„ãªãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

ã€åŸºæœ¬æƒ…å ±ã€‘ç·å£ã‚³ãƒŸæ•°:{total}ä»¶
ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ Top15ã€‘{pos_summary}
ã€ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ Top15ã€‘{neg_summary}
ã€ä»£è¡¨çš„ãªå£ã‚³ãƒŸï¼ˆä½è©•ä¾¡å„ªå…ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰ã€‘
{sample_text}

## è¨˜è¿°ãƒ«ãƒ¼ãƒ«ï¼ˆå³å®ˆï¼‰
- ä¸»è¦³çš„ãªè©•ä¾¡èªï¼ˆã€Œå¼·ã„ã€ã€Œå„ªã‚Œã¦ã„ã‚‹ã€ã€Œèª²é¡Œã€ã€Œäººæ°—ã€ã€Œæ”¯æŒã•ã‚Œã¦ã„ã‚‹ã€ãªã©ï¼‰ã¯ä½¿ã‚ãªã„
- å¿…ãšå£ã‚³ãƒŸä»¶æ•°ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾ä»¶æ•°ãƒ»å‰²åˆãªã©ã®æ•°å€¤ã‚’æ ¹æ‹ ã¨ã—ã¦ç¤ºã™
  - è‰¯ã„ä¾‹ï¼šã€Œã€‡ã€‡ã‚’è©•ä¾¡ã™ã‚‹å£°ãŒâ–³ä»¶ã¿ã‚‰ã‚Œã‚‹ã€ã€Œâ–¡â–¡ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒâ–³ä»¶ã®å£ã‚³ãƒŸã«å‡ºç¾ã—ã¦ã„ã‚‹ã€
  - è‰¯ã„ä¾‹ï¼šã€Œã€‡ã€‡ã‚’æŒ‡æ‘˜ã™ã‚‹å£°ãŒâ–³ä»¶ã‚ã‚Šã€æ”¹å–„ã™ã‚‹ã“ã¨ã§é¡§å®¢ä½“é¨“ãŒå‘ä¸Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€
  - æ‚ªã„ä¾‹ï¼šã€Œã€‡ã€‡ãŒé«˜ãè©•ä¾¡ã•ã‚Œã¦ã„ã‚‹ã€ã€Œã€‡ã€‡ãŒèª²é¡Œã§ã™ã€
- headline ã¯ã€Œã€‡ã€‡ã®å£°ãŒå¤šã„é£²é£Ÿä½“é¨“ã€ãªã©ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨€ãˆã‚‹äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã®è¡¨ç¾ã«ã™ã‚‹
- summary ã¯å£ã‚³ãƒŸå…¨ä½“ã®å‚¾å‘ã‚’ä»¶æ•°ãƒ»å‰²åˆãƒ™ãƒ¼ã‚¹ã§è¦ç´„ã™ã‚‹ï¼ˆ150æ–‡å­—ç¨‹åº¦ï¼‰
- strengths ã¯ä¸»è¦ãªãƒã‚¸ãƒ†ã‚£ãƒ–è©•ä¾¡ã‚’2ã€œ3ä»¶ã«ã¾ã¨ã‚ã‚‹ï¼ˆç¶²ç¾…çš„ã«ã—ãªãã¦ã‚ˆã„ï¼‰
- weaknesses ã¯æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ç‚¹ã‚’ç¶²ç¾…çš„ã«åˆ—æŒ™ã™ã‚‹ï¼ˆä»¶æ•°ãŒå°‘ãªãã¦ã‚‚æ”¹å–„ä¾¡å€¤ã®ã‚ã‚‹æŒ‡æ‘˜ã¯å¿…ãšå«ã‚ã‚‹ï¼‰
  - ä¾‹ï¼šã‚¢ã‚¯ã‚»ã‚¹ãƒ»ã‚ã‹ã‚Šã«ãã•ãƒ»å¾…ã¡æ™‚é–“ãƒ»ä¾¡æ ¼ãƒ»æƒ…å ±ä¸è¶³ãªã©ã‚‚å¯¾è±¡
- strengths ã® description ã¯ã€Œã€‡ã€‡ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒâ–³ä»¶å‡ºç¾ã€ã€Œã€‡ã€‡ã‚’è©•ä¾¡ã™ã‚‹å£ã‚³ãƒŸãŒâ–³ä»¶ã€ãªã©ä»¶æ•°ã‚’å«ã‚ã‚‹
- weaknesses ã® description ã¯ã€Œã€‡ã€‡ã‚’æŒ‡æ‘˜ã™ã‚‹å£°ãŒâ–³ä»¶ã¿ã‚‰ã‚Œã‚‹ã€ãªã©ä»¶æ•°ã‚’å«ã‚ã‚‹

ä»¥ä¸‹ã®å½¢å¼ã§JSONã®ã¿å‡ºåŠ›ï¼ˆå‰ç½®ãä¸è¦ï¼‰:
{{"headline":"20æ–‡å­—ä»¥å†…","summary":"150æ–‡å­—ç¨‹åº¦","strengths":[{{"title":"è¦³ç‚¹","description":"ä»¶æ•°ã‚’å«ã‚€å®¢è¦³çš„èª¬æ˜"}}],"weaknesses":[{{"title":"è¦³ç‚¹","description":"ä»¶æ•°ã‚’å«ã‚€å®¢è¦³çš„èª¬æ˜"}}]}}"""

    try:
        json_match = re.search(r"\{.*\}", _generate(prompt), re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
    except Exception as e:
        print(f"    âš ï¸ é¡§å®¢ä½“é¨“ä¾¡å€¤åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    return {"headline": "åˆ†æã‚¨ãƒ©ãƒ¼", "summary": "", "strengths": [], "weaknesses": []}


# ---------------------------------------------------------------------------
# æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰åŒ–åˆ†æ
# ---------------------------------------------------------------------------

def _is_recent(date_str: str) -> bool | None:
    """ç›´è¿‘3ãƒ¶æœˆã‹ã©ã†ã‹åˆ¤å®šã€‚None = åˆ¤å®šä¸èƒ½ï¼ˆã©ã¡ã‚‰ã«ã‚‚å«ã‚ãªã„ï¼‰ã€‚"""
    if not date_str:
        return None
    # ã€Œæœ€çµ‚ç·¨é›†: X ã‹æœˆå‰ã€ãªã©ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»
    s = re.sub(r'^æœ€çµ‚ç·¨é›†[:ï¼š]\s*', '', date_str.strip())
    # æ—¥ãƒ»æ™‚é–“ãƒ»åˆ†ãƒ»é€±é–“å‰ â†’ ç›´è¿‘3ãƒ¶æœˆä»¥å†…
    if re.search(r'\d+\s*(æ—¥|æ™‚é–“|åˆ†|é€±é–“)å‰', s):
        return True
    # ã‹æœˆå‰ / ãƒ¶æœˆå‰ / ãƒµæœˆå‰
    m = re.search(r'(\d+)\s*[ã‹ãƒ¶ãƒµ]æœˆå‰', s)
    if m:
        return int(m.group(1)) <= 3
    # å¹´å‰ â†’ ãã‚Œä»¥å‰
    if re.search(r'\d+\s*å¹´å‰', s):
        return False
    # çµ¶å¯¾æ—¥ä»˜
    ym = _extract_year_month(s)
    if ym:
        try:
            d = datetime.strptime(ym, "%Y-%m")
            cutoff = datetime(2025, 11, 24)
            return d >= cutoff
        except Exception:
            pass
    return None


def _analyze_timeseries_keywords(reviews: list[dict], all_keywords: list[dict]) -> dict:
    """ç›´è¿‘3ãƒ¶æœˆ vs ãã‚Œä»¥å‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾ç‡ã‚’æ¯”è¼ƒã€‚"""
    print("  ğŸ“… æ™‚ç³»åˆ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰åŒ–ã‚’åˆ†æä¸­...")
    recent = [r for r in reviews if _is_recent(r.get("date", "")) is True]
    older  = [r for r in reviews if _is_recent(r.get("date", "")) is False]

    def count_rate(word: str, review_list: list) -> tuple[int, float]:
        c = sum(1 for r in review_list if word in r.get("text", ""))
        rate = round(c / len(review_list) * 100, 1) if review_list else 0.0
        return c, rate

    changes = []
    for kw in all_keywords[:30]:
        word = kw["word"]
        rc, rr = count_rate(word, recent)
        oc, or_ = count_rate(word, older)
        diff = round(rr - or_, 1)
        changes.append({
            "word": word,
            "sentiment": kw.get("sentiment", "neutral"),
            "recent_count": rc,
            "recent_rate": rr,
            "older_count": oc,
            "older_rate": or_,
            "change": diff,
        })

    changes.sort(key=lambda x: abs(x["change"]), reverse=True)
    return {
        "recent_count": len(recent),
        "older_count": len(older),
        "keywords": changes,
    }


# ---------------------------------------------------------------------------
# æ„Ÿå‹•ã®7é¡å‹åˆ†æ
# ---------------------------------------------------------------------------

KANDO_BATCH = 15

def _analyze_kando(reviews: list[dict]) -> dict:
    """æ„Ÿå‹•ã®7é¡å‹ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã€ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã€‚"""
    print("  ğŸ­ æ„Ÿå‹•ã®7é¡å‹ã‚’åˆ†æä¸­...")

    all_scores: dict[str, list[int]] = {t: [] for t in KANDO_TYPES}
    detection: dict[str, int] = {t: 0 for t in KANDO_TYPES}
    total_batches = (len(reviews) + KANDO_BATCH - 1) // KANDO_BATCH

    for i in range(0, len(reviews), KANDO_BATCH):
        batch = reviews[i: i + KANDO_BATCH]
        batch_num = i // KANDO_BATCH + 1
        print(f"    ğŸ“¦ æ„Ÿå‹•åˆ†æãƒãƒƒãƒ {batch_num}/{total_batches}...")

        rows = "\n".join(f"[{j}] {r['text'][:200]}" for j, r in enumerate(batch))
        prompt = f"""ä»¥ä¸‹ã®é£²é£Ÿåº—å£ã‚³ãƒŸã‚’ã€Œæ„Ÿå‹•ã®7é¡å‹ã€ã§è©•ä¾¡ã—ã€JSONé…åˆ—ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

## 7é¡å‹ï¼ˆå„0ã€œ5ç‚¹ï¼‰
â‘  thresholdï¼ˆã—ãã„å€¤çªç ´ï¼‰: æœŸå¾…ã‚’è¶…ãˆã‚‹åœ§å€’çš„ä½“é¨“ãƒ»æœ€ä¸Šç´šè¡¨ç¾
â‘¡ surpriseï¼ˆæ„å¤–æ€§ï¼‰: äºˆæœŸã—ãªã‹ã£ãŸå¬‰ã—ã„ä½“é¨“ãƒ»ã‚µãƒ—ãƒ©ã‚¤ã‚º
â‘¢ resonanceï¼ˆå…±é³´ãƒ»å…±æ„Ÿï¼‰: è¨˜æ†¶ãƒ»äººç”Ÿãƒ»ç‰©èªã¨ã®å…±é³´ãƒ»æ‡ã‹ã—ã•
â‘£ rescueï¼ˆæ•‘æ¸ˆï¼‰: å›°ã£ãŸæ™‚ã®åŠ©ã‘ãƒ»ã‚¹ã‚¿ãƒƒãƒ•ã®æ°—é£ã„ãƒ»å¯¾å¿œ
â‘¤ aweï¼ˆå´‡é«˜ï¼‰: éæ—¥å¸¸ãƒ»ä¸–ç•Œè¦³ãƒ»ç•°ç©ºé–“ã¸ã®åœ§å€’ãƒ»ç•æ•¬
â‘¥ participationï¼ˆå‚åŠ ï¼‰: ä½“é¨“ã¸ã®å‚åŠ ãƒ»ä¸€ä½“æ„Ÿãƒ»ä¸»ä½“çš„é–¢ä¸
â‘¦ growthï¼ˆæˆé•·ï¼‰: ãƒªãƒ”ãƒ¼ãƒˆãƒ»æ™‚é–“å¤‰åŒ–ãƒ»æˆé•·ãƒ»å­£ç¯€å¤‰åŒ–

ã‚¹ã‚³ã‚¢åŸºæº–: 0=è¨€åŠãªã— / 1=æ›–æ˜§ / 2=æ˜ç¢ºã ãŒå¼±ã„ / 3=æ˜ç¢º+æ„Ÿæƒ… / 4=å¼·ã„æ„Ÿæƒ…+å…·ä½“ä¾‹ / 5=åœ§å€’çš„

å£ã‚³ãƒŸ:
{rows}

å‡ºåŠ›ï¼ˆJSONé…åˆ—ã®ã¿ã€idã¯0å§‹ã¾ã‚Šï¼‰:
[{{"id":0,"threshold":0,"surprise":0,"resonance":0,"rescue":0,"awe":0,"participation":0,"growth":0}}]"""

        try:
            json_match = re.search(r'\[.*\]', _generate(prompt), re.DOTALL)
            if json_match:
                for item in json.loads(json_match.group()):
                    idx = item.get("id", 0)
                    if 0 <= idx < len(batch):
                        for t in KANDO_TYPES:
                            score = int(item.get(t, 0))
                            all_scores[t].append(score)
                            if score > 0:
                                detection[t] += 1
            time.sleep(2)
        except Exception as e:
            print(f"    âš ï¸ ã‚¨ãƒ©ãƒ¼ï¼ˆãƒãƒƒãƒ {batch_num}ï¼‰: {e}")
            for _ in batch:
                for t in KANDO_TYPES:
                    all_scores[t].append(0)

    total = len(reviews)
    aggregated = {}
    for t in KANDO_TYPES:
        scores = all_scores[t]
        avg = round(sum(scores) / len(scores), 2) if scores else 0.0
        aggregated[t] = {
            "label": KANDO_LABELS[t],
            "score": avg,
            "detection_rate": round(detection[t] / total * 100, 1) if total else 0.0,
            "review_count": detection[t],
            "is_reliable": detection[t] >= 3,
        }

    sorted_types = sorted(KANDO_TYPES, key=lambda t: aggregated[t]["score"], reverse=True)
    strengths  = sorted_types[:2]
    weaknesses = sorted_types[-2:]

    # AI ã‚³ãƒ³ã‚µãƒ«ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
    radar_summary = "\n".join(
        f"- {aggregated[t]['label']}: {aggregated[t]['score']:.1f}/5 (æ¤œå‡ºç‡{aggregated[t]['detection_rate']}%)"
        for t in KANDO_TYPES
    )
    top_reviews_text = "\n".join(f"ã€Œ{r['text'][:100]}ã€" for r in reviews[:10] if r.get("text"))

    comment_prompt = f"""ä»¥ä¸‹ã®å£ã‚³ãƒŸåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¨ã«ã€æ„Ÿå‹•ã®7é¡å‹ã”ã¨ã®åˆ†æçµæœã‚’å®¢è¦³çš„ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

## åˆ†æãƒ‡ãƒ¼ã‚¿
{radar_summary}

## ä»£è¡¨çš„ãªå£ã‚³ãƒŸ
{top_reviews_text}

## è¨˜è¿°ãƒ«ãƒ¼ãƒ«
1. ä¸»è¦³çš„ãªè©•ä¾¡èªï¼ˆã€Œå¼·ã„ã€ã€Œå„ªã‚Œã¦ã„ã‚‹ã€ã€Œèª²é¡Œã§ã™ã€ãªã©ï¼‰ã¯ä½¿ã‚ãªã„
2. ãƒ‡ãƒ¼ã‚¿ã‚’æ ¹æ‹ ã«ã—ãŸå®¢è¦³çš„ãªè¡¨ç¾ã®ã¿ä½¿ã†
   - è‰¯ã„ä¾‹: ã€Œã€‡ã€‡ã®ã‚¹ã‚³ã‚¢ãŒæœ€ã‚‚é«˜ãã€ã€‡ã€‡ã‚’è©•ä¾¡ã™ã‚‹å£°ãŒè¤‡æ•°ã¿ã‚‰ã‚Œã‚‹ã€
   - è‰¯ã„ä¾‹: ã€Œã€‡ã€‡ã‚’æŒ‡æ‘˜ã™ã‚‹å£°ãŒè¤‡æ•°ã‚ã‚Šã€æ”¹å–„ã™ã‚‹ã“ã¨ã§é¡§å®¢ä½“é¨“ãŒå‘ä¸Šã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€
   - æ‚ªã„ä¾‹: ã€Œã€‡ã€‡ãŒå¼·ãæ”¯æŒã•ã‚Œã¦ã„ã‚‹ã€ã€Œã€‡ã€‡ãŒèª²é¡Œã§ã™ã€
3. ã‚¹ã‚³ã‚¢ã¨æ¤œå‡ºç‡ã®æ•°å€¤ã‚’ç©æ¥µçš„ã«å¼•ç”¨ã™ã‚‹
4. å£ã‚³ãƒŸã®å…·ä½“çš„ãªè¡¨ç¾ã‚’å¼•ç”¨ã—ã¦æ ¹æ‹ ã‚’ç¤ºã™
5. å…¨ä½“ã§300ã€œ500æ–‡å­—

åˆ†æçµæœãƒ†ã‚­ã‚¹ãƒˆã®ã¿å‡ºåŠ›ï¼ˆå‰ç½®ãä¸è¦ï¼‰:"""

    try:
        ai_comment = _generate(comment_prompt)
    except Exception as e:
        ai_comment = f"ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"

    return {
        "aggregated": aggregated,
        "strengths": strengths,
        "weaknesses": weaknesses,
        "ai_comment": ai_comment,
        "total_analyzed": total,
    }


# ---------------------------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ---------------------------------------------------------------------------

def _extract_year_month(date_str: str) -> str | None:
    if not date_str:
        return None
    patterns = [
        (r"(\d{4})[/-](\d{1,2})", lambda m: f"{m.group(1)}-{int(m.group(2)):02d}"),
        (r"(\d{4})å¹´\s*(\d{1,2})æœˆ", lambda m: f"{m.group(1)}-{int(m.group(2)):02d}"),
        (
            r"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\w*\s+(\d{4})",
            lambda m: f"{m.group(2)}-{_month_to_num(m.group(1)):02d}",
        ),
    ]
    for pattern, formatter in patterns:
        m = re.search(pattern, date_str, re.IGNORECASE)
        if m:
            try:
                return formatter(m)
            except Exception:
                continue
    return None


def _month_to_num(abbr: str) -> int:
    return {"jan":1,"feb":2,"mar":3,"apr":4,"may":5,"jun":6,
            "jul":7,"aug":8,"sep":9,"oct":10,"nov":11,"dec":12}.get(abbr.lower()[:3], 1)
